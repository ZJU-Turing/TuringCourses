# 数据挖掘导论
<div class="badges">
<span class="badge cs-badge">CS 专业模块-计算机科学</span>
</div>

## 课程学习内容

这门课主要包括如下内容：

* 数据预处理，包括数据清洗、数据补全、数据降噪、数据降维、数据离散化等
* 聚类，包括分割方法、分层方法、基于密度的方法、基于网格的方法，以及聚类效果评估等
* 分类，包括决策树、贝叶斯方法、基于规则的方法，模型评估与方法，以及从许多弱的小分类器组合出强的大分类器的组装方法。
* 频繁模式挖掘，包括 Apiori 算法和 FP-growth 算法，以及频繁模式评估
* 异常点检测

## 任课教师

=== "李石坚"

    在讲课方面，李石坚的讲课非常认真，会愿意讲得很细致，但是讲课技巧并不好，并不容易让人听懂。大概可以理解为他会讲一些算法细节，但是他本人对于一些算法细节并不清楚（不过大致过程以及大部分细节还是清楚的），综合评价为郑文庭。

    李石坚上课从不点名。有一次上课时只来了 3 个人，他还是淡然自若地直接上课而不点名。但是他给分却不心慈手软，往往每次作业打分都会比互评分低很多，可能你的互评分 98+，最后师评分 80+，90 并不会乱给。不过也不用担心付出得不到回报，笔者所知的写得认真的同学都是 90+。

=== "张东祥"

    讲课认真，内容干货也比较多，但似乎听的人不多。前两节课会劝退，“对于只是想拿学分的同学，这门课的性价比不高”，但从给分、时间投入和产出以及课程体验上来看，还是值得一选的。
    
    最后大作业答辩的时候比较给面子。对于答不上来/答得不太正确的问题，老师或助教也会给出自己的看法。对于性能指标略弱的同学会多提一些问题，试图多给一些创新性方面的分。总之大作业的评价标准还是比较客观的，基本上一分耕耘一分收获。

## 分数构成

=== "李石坚"

    20 级 lsj 班具有如下的作业要求：

    * 平时作业（40%）
        - 一共 4 次作业，每次占总评的 10 分
        - 前 3 次为实验，分别关于数据预处理、聚类/分类、频繁模式挖掘，需要提交代码和报告
        - 最后 1 次为论文阅读，需要选择一篇经典的数据挖掘顶会论文，提交论文阅读报告和 PPT
        - 每次作业都会互评，互评占 50%，师评占 50%。如果错过互评提交时间，互评分直接为 0，但是可以向老师哭诉让他适当提高师评分
    * 期末大程（60%）
        - 选择 Kaggle、KDD Cup、天池等国内外数据挖掘比赛中的入门级选题开展（可以做练习赛）
        - 事实上除了比赛数据集也可以选择别的数据集（比如笔者就选了非比赛数据集）
        - 可以提交开题报告，让老师判断工作量是否恰当
        - 关键在于要将上课所讲的方法都实践一遍，即数据预处理、聚类分类和可视化、频繁模式挖掘、离群点判别等
        - 最后需要提交代码和报告，可以不提交开题报告
        - 评分依然包括互评，10% 的互评，90% 的师评
        - 还有一种选择，选择一个比赛打榜，打到全球前 5%，经过老师判定后可以直接得到满绩
    * 加分（?%）
        - 论文阅读作业中得分较高的同学可以选择在最后一节课上台 pre，总评 +1
        - 在第一节课可以报名成为课代表，事实上将会成为助教，变成老师的免费打工人；老师表示将会凭业绩给你总评加分；20 级的笔者担任了课代表，最后满绩了，但是并不知道加了多少
        - 以上的加分不会显式地体现，老师表示将作为他作业打分的考量，即你得分的结果已经包含了加分

=== "张东祥"

    * 平时作业 + 两次点名（40%）
        - 一共 4 次作业
        - 前 3 次为实验，分别关于数据预处理、分类、聚类，只需要提交代码
        - 最后 1 次为论文阅读，指定的两篇论文中选一篇写阅读报告
        - 无互评，方法对了就给满了，对指标要求不高，无需花时间调参
        - 点了 3 次名，第一次在钉钉群里点，发现点到人数远远超过教室里的人数，于是第二、三次改成在纸上签到
    * 期末大程（60%）
        - 20 级有 2 个可选的项目。每道题目会提供一个 baseline，跑通 baseline 好像就会有基本分了，然后可以基于 baseline 改进或者用其他自己的方法提高性能。一般可选的题目有两类，一类是指定的 Kaggle 上的比赛题，一类是深度学习的题。据老师所说，一般选 Kaggle 那题的人比较多。个人感觉有一定基础的话做深度学习的那个题也并不难，把 metrics 做得漂亮一些可能也会更出彩。
            - 20 级的两道题：
                1. Kaggle 比赛题：https://www.kaggle.com/c/store-sales-time-series-forecasting/data
                2. 蛋白质相似性检索：使用深度学习模型拟合传统蛋白质相似性计算过程，利用训练后的模型提取查询蛋白质序列与蛋白质库中其余蛋白质序列的嵌入表示（embedding）并计算距离（distance）。通过蛋白质之间的距离排序得到最相似的K条蛋白质。注：计算嵌入表示的距离的方式必须采用欧氏距离/余弦距离等低代价的可并行方式。训练集：3000 条蛋白质序列，标签为 3000 条序列之间的传统蛋白质相似性距离矩阵。测试集：2000 条蛋白质序列，也具有传统蛋白质相似性距离矩阵，500条作为查询蛋白质序列，剩余 1500 条为待查询蛋白质序列。评测指标：用 P@k (k=1, 5, 10, 50, 100, 500)（Top-k Precision）来评估模型的效果。Baseline 算法：CNNED (SIGIR 2020)。
        - 评分标准包括测试指标，工作量，创新性等
        - 需要提交代码，报告，答辩ppt
        - 答辩前一天需要将答辩视频（3-5分钟）发给助教，现场答辩只有Q&A环节，一般5-10分钟

## 参考资料

### 课程教材

*《数据挖掘概念与技术》Data Mining: Concepts and Techniques*

作者为韩家炜 (Jiawei Han) 以及 Micheline Kamber。这本教材讲得还是挺好的，可以看看，笔者直接看的电子书。

### 其他资料

笔者认为 PPT 和教材已经足够通过这门课，但除此之外李石坚还给出了如下的参考资料：

- T. Hastie, R. Tibshirani, and J. Friedman, The elements of statistical learning: data mining, inference and prediction: Springer, 2009.
- C. Bishop and SpringerLink, Pattern recognition and machine learning vol. 4: Springer New York, 2006.
- S. Boyd and L. Vandenberghe, Convex optimization: Cambridge Univ Pr, 2004.
- P. Harrington, Machine learning in action vol. 5: Manning Greenwich, CT, 2012.

## 学习建议

在听课体验上，据了解应该是张东祥胜过李石坚，查老师给分上李石坚胜过张东祥，但是实际情况未知。在这里笔者并不给出倾向哪一方的推荐。

总体上，这门课应该更偏向于应用与实践，比较锻炼编程实践能力。正如其名，笔者认为是专业课中和数据打交道比较多的一门课，对数据进行适当的处理和分析得到一定的结论，应当也是现在利用深度学习解决问题所必备的一个技能，如果做数模、量化之类的应该更是直接需要。

总而言之，自学吧，笔者感觉这门课的 workload 并不算很大，用来凑专业模块课学分并且同时学到一些知识还是比较适合的。
